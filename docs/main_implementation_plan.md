### **Complete Implementation Plan (6 Weeks)**

**Goal:** Benchmark a **fully custom framework** (C++/OpenMP data loading + CUDA kernels + C/MPI) against PyTorch DDP for MNIST/CIFAR-10.

---

### **1. Revised Project Structure**

```bash
your_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mnist/               # Raw MNIST files (train-images-idx3-ubyte, etc.)
â”‚   â””â”€â”€ preprocessed/        # Normalized/processed data (generated by your C++ loader)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader/         # Custom data loading (C++/OpenMP)
â”‚   â”‚   â”œâ”€â”€ DataLoader.cpp   # Parallel data loading & augmentation
â”‚   â”‚   â”œâ”€â”€ DataLoader.h
â”‚   â”‚   â””â”€â”€ bindings.cpp     # pybind11 Python interface
â”‚   â”‚
â”‚   â”œâ”€â”€ model/               # CUDA-based model
â”‚   â”‚   â”œâ”€â”€ cuda_kernels.cu  # CUDA kernels (e.g., matmul, ReLU, loss)
â”‚   â”‚   â”œâ”€â”€ Model.cpp        # Wrapper for CUDA kernels (CPU-GPU bridge)
â”‚   â”‚   â””â”€â”€ Model.h
â”‚   â”‚
â”‚   â”œâ”€â”€ mpi/                 # MPI communication (gradient sync)
â”‚   â”‚   â”œâ”€â”€ gradient_sync.c  # C/MPI gradient aggregation
â”‚   â”‚   â””â”€â”€ mpi_wrapper.cpp  # Python bindings for MPI functions
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â”œâ”€â”€ checkpoint.cpp   # Save/load model weights
â”‚       â””â”€â”€ timer.hpp        # Performance timing
â”‚
â”œâ”€â”€ python/                  # Python training scripts
â”‚   â”œâ”€â”€ train.py             # Main training loop
â”‚   â””â”€â”€ config.py            # Hyperparameters (batch size, LR)
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ pytorch_ddp/         # PyTorch baseline
â”‚   â”‚   â””â”€â”€ train_pytorch.py
â”‚   â””â”€â”€ train_custom.py      # Your frameworkâ€™s entry point
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh             # Compile C++/CUDA/MPI code
â”‚   â””â”€â”€ launch.slurm         # Slurm script for HPC
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README.md            # Build/run instructions
â”‚
â”œâ”€â”€ Makefile                 # Compilation rules
â””â”€â”€ requirements.txt         # Python dependencies (pybind11, numpy)
```

---

### **2. Streamlined Implementation Stages**

#### **Stage 1: Setup & Custom Data Loader (1.5 Weeks)**

**Objective:** Replace PyTorch DataLoader with a C++/OpenMP solution.

- **Tasks:**
  1. **Implement MNIST Data Loader in C++:**
     - Read raw MNIST files (`train-images-idx3-ubyte`).
     - Use OpenMP to parallelize data augmentation (e.g., normalization, cropping).
     - Example:
       ```cpp
       // DataLoader.cpp
       #include <omp.h>
       void load_data(float* images, int num_samples) {
           #pragma omp parallel for
           for (int i = 0; i < num_samples; i++) {
               // Load and preprocess image[i]
           }
       }
       ```
  2. **Create Python Bindings with pybind11:**
     - Expose the C++ loader to Python:
       ```cpp
       // bindings.cpp
       #include <pybind11/pybind11.h>
       PYBIND11_MODULE(data_loader, m) {
           m.def("load_data", &load_data);
       }
       ```
  3. **Validate Speed:**
     - Compare throughput (samples/sec) against PyTorch DataLoader.

**Deliverable:**

- A custom data loader that matches/exceeds PyTorchâ€™s performance.

---

#### **Stage 2: CUDA Model & Training Loop (2 Weeks)**

**Objective:** Implement a simple model (e.g., CNN) with CUDA kernels.

- **Tasks:**

  1. **Write CUDA Kernels:**
     - Forward/backward passes for layers (e.g., Conv2D, ReLU, Softmax).
     - Example CUDA kernel for ReLU:
       ```cpp
       // cuda_kernels.cu
       __global__ void relu(float *input, float *output, int N) {
           int idx = blockIdx.x * blockDim.x + threadIdx.x;
           if (idx < N) output[idx] = fmaxf(0.0f, input[idx]);
       }
       ```
  2. **Integrate with Python:**
     - Use `ctypes` or `pybind11` to call CUDA kernels from Python.
  3. **Implement Training Loop:**

     - Manually compute gradients (e.g., using finite differences for simplicity).
     - Example workflow:

       ```python
       # train.py
       from data_loader import load_data
       from model import forward, backward

       images, labels = load_data()  # Custom C++ loader
       for epoch in epochs:
           preds = forward(images)  # CUDA kernel
           loss = compute_loss(preds, labels)
           gradients = backward(loss)
       ```

**Deliverable:**

- A GPU-accelerated model trained on MNIST.

---

#### **Stage 3: C/MPI Gradient Synchronization (1.5 Weeks)**

**Objective:** Aggregate gradients across nodes using C/MPI (not PyTorch).

- **Tasks:**
  1. **Implement Gradient Sync in C:**
     - Use `MPI_Allreduce` for GPU-resident gradients (CUDA-aware MPI):
       ```c
       // gradient_sync.c
       #include <mpi.h>
       void sync_gradients(float *gradients, int size) {
           MPI_Allreduce(MPI_IN_PLACE, gradients, size, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
       }
       ```
  2. **Integrate with Python Training Loop:**
     - Load the compiled C/MPI library using `ctypes`:
       ```python
       # train.py
       from ctypes import CDLL
       mpi_lib = CDLL("./gradient_sync.so")
       mpi_lib.sync_gradients(gradients.ctypes.data, gradients.size)
       ```

**Deliverable:**

- Distributed training with custom MPI gradient synchronization.

---

#### **Stage 4: Benchmarking & Report (1 Week)**

**Objective:** Compare your framework against PyTorch DDP.

- **Metrics:**
  - **Training Time:** Total time per epoch.
  - **Data Loading Speed:** Samples/sec.
  - **GPU Utilization:** `nvidia-smi` logs.
- **Experiments:**
  - Train MNIST on 1/2/4 GPUs with both frameworks.

**Deliverable:**

- A report showing your frameworkâ€™s performance vs. PyTorch.

---

### **3. Key Changes from Original Plan**

1. **No PyTorch Dependency:**
   - Custom data loading (C++/OpenMP), model (CUDA), and gradient sync (C/MPI).
2. **Simplified Model:**
   - Focus on a small CNN or logistic regression to reduce CUDA complexity.
3. **Build System:**
   - Use a `Makefile` to compile C++/CUDA/MPI code into shared libraries.

---

### **4. Example Build Command**

```bash
# scripts/build.sh
g++ -O3 -fopenmp -shared -std=c++11 -fPIC src/data_loader/DataLoader.cpp -o data_loader.so
nvcc -Xcompiler -fPIC -shared src/model/cuda_kernels.cu -o cuda_kernels.so
mpicc -shared src/mpi/gradient_sync.c -o gradient_sync.so
```

---

### **Why This Works**

- **Focus on Parallel Computing:** Custom data loading (OpenMP), GPU acceleration (CUDA), and distributed training (MPI) align with your class goals.
- **Avoid PyTorch Overhead:** Direct control over performance-critical components.
- **Real-World HPC Practices:** Uses CUDA-aware MPI and GPU-direct communication.

This plan ensures you gain hands-on experience with **CUDA, MPI, and OpenMP** while delivering a meaningful benchmark against PyTorch. Letâ€™s go! ðŸš€
